%!TEX root =  main.tex
\chapter*{Chapter 7 The Upper Confidence Bound Algorithm}
\label{sec:7}

\noindent\textbf{7.1} (\textsc{Concentration for sequences of random length})
    In this exercise, we investigate one of the more annoying challenges when analyzing sequential algorithms.
    Let $X_{1}, X_{2}, \ldots$ be a sequence of independent standard Gaussian random variables defined on probability space $(\Omega, \mathcal{F}, \mathbb{P})$.
    Suppose that $T: \Omega \rightarrow\{1,2,3, \ldots\}$ is another random variable, and let $\hat{\mu}=\sum_{t=1}^{T} X_{t} / T$ be the empirical mean based on $T$ samples.

%     \begin{enumerate}[(a)]
%     \item \begin{align*}
%     P &=(\hat{\mu}-\mu\ge\sqrt{\dfrac{2log(1/\delta)}{T}})\\
%     &=\sum^{\infty}_{n=1}E[\{T=n\}\|\{\hat{\mu}-\mu\ge\sqrt{\dfrac{2log(1/\delta)}{T}}\}]\\
%     &=\sum^{\infty}_{n=1}E[\|\{T=n\}\delta]\\
%     &=\delta\sum^{\infty}_{n=1}P(T=n)\\
%     &=\delta
%     \end{align*}

%     \item
%  $\hat{\mu}-\mu=\frac{1}{n}\sum^{\infty}_{t=1}(X_t-\mu)\ge\sqrt{\dfrac{2-\log(1/\delta)}{T}}$

%     T=min(n)

%     $E_t=\|\{T=t\}$ is $\mathcal{F}_t$-measurable

%     \item Show that
%     $$\mathbb{P}\left(\hat{\mu}-\mu \geq \sqrt{\frac{2 \log (T(T+1) / \delta)}{T}}\right) \leq \delta.$$

%     \begin{proof}
%         Note that
%         \begin{align*}
%             P(\hat{\mu}-\mu\ge\sqrt{\dfrac{2log(T(T+1)/\delta}{T}}) &\le P(\bigcup^{\infty}_{n=1}\hat{\mu}-\mu\ge\sqrt{\dfrac{2log(n(n+1)/\delta}{n}})\\
%             &\le \sum^{\infty}_{n=1}P(\hat{\mu}-\mu\ge\sqrt{\dfrac{2log(n(n+1)/\delta}{n}})\\
%             &\le \sum^{\infty}_{n=1} \dfrac{\delta}{n(n+1)}\\
%             &\le \delta
%         \end{align*}
%     \end{proof}
% \end{enumerate}

\noindent\textbf{7.2} (\textsc{Relaxing the subgaussian assumption}) In this chapter, we assumed the pay-off distributions were 1-subgaussian. The purpose of this exercise is to relax this assumption. 
\begin{enumerate}
    \item[(a)] First suppose that $\sigma^2$ is a known constant and that $\nu \in \cE_{SG}^k (\sigma^2)$. Modify the UCB algorithm and state and prove an analogue of Theorems 7.1 and 7.2 for this case.
    \item[(b)] Now suppose that $\nu =(P_i)_{i=1}^k $ is chosen so that $P_i$ is $\sigma_i$-subgaussian where $(\sigma_i^2)_{i=1}^k$ are known. Modify the UCB algorithm and state and prove an analogue of Theorems 7.1 and 7.2 for this case.
    \item[(c)] If you did things correctly, the regret bound in the previous part should not depend on the values $\set{\sigma_i^2:\Delta_i=0}$. Explain why not.
\end{enumerate}

\begin{proof}
\begin{enumerate}
    \item[(a)] We modify the UCB algorithm by replacing $\sqrt{\frac{2\log (1/\delta)}{T_i(t-1)}}$ with $\sqrt{\frac{2\sigma^2 \log (1/\delta)}{T_i(t-1)}}$ in Eq. (7.2) on page 85 of the book. 
    Recall the reward $X_{t,i}$ of each arm $i$ at time $t$ is $\sigma$-subgaussian. According to the properties of subgaussian random variables, we have $\PP{\abs{\hat{\mu}_{is} - \mu_i }\ge \sqrt{\frac{2\sigma^2 \log (1/\delta)}{s}} } \le 2\delta$ for each arm $i$ and $s>0$. 

    Define event $\cF_t = \set{\exists i \in [k]: \abs{\hat{\mu}_{i}(t) - \mu_i }\ge \sqrt{\frac{2\sigma^2 \log (1/\delta)}{T_i(t-1)}}}$. 
    The regret can be decomposed by
    \begin{align*}
        R_n &= \sum_{i:\Delta_i>0}\Delta_i \EE{ \sum_{t=1}^n \bOne{A_t = i} } \\
        &\le \sum_{i:\Delta_i>0}\Delta_i \set{ \EE{ \sum_{t=1}^n \bOne{A_t = i, \cF_t} } +  \EE{ \sum_{t=1}^n \bOne{A_t = i, \neg \cF_t} } } \\
        &\le \sum_{i:\Delta_i>0}\Delta_i \set{ \EE{ \sum_{t=1}^n \bOne{A_t = i, \neg \cF_t} } +  2nk\delta }\\
        &\le \sum_{i:\Delta_i>0}\Delta_i \set{ \EE{ 1+ \sum_{s=1}^{n-1} \bOne{ \mu_i + 2\sqrt{\frac{2\sigma^2 \log (1/\delta)}{s}} } \ge \mu_1 }  +  2nk\delta } \\
        &= \sum_{i:\Delta_i>0}\Delta_i \bracket{ 3 + \frac{16\sigma^2 \log n}{\Delta_i^2}} \,,
    \end{align*}
    where the last inequality holds by choosing $\delta=1/n^2$.  Thus the analogue of Theorems 7.1 for this case has been proved. Similar to the proof technique of Theorem 7.2, by setting $\Delta = 4\sqrt{\frac{k\sigma^2 \log n}{n}}$, we have $R_n \le 8\sqrt{nk\sigma^2 \log n} + 3\sum_{i:\Delta_i >0} \Delta_i$. Thus the analogue of Theorems 7.2 for this case has also been proved. 

    \item[(b)] For this case, we modify the UCB algorithm by replacing $\sqrt{\frac{2\log (1/\delta)}{T_i(t-1)}}$ with $\sqrt{\frac{2\sigma_i^2 \log (1/\delta)}{T_i(t-1)}}$ in Eq. (7.2) on page 85 of the book. The same proof techniques with part (a) can be used to derive the regret upper bound of order $R_n = \sum_{i:\Delta_i>0}\Delta_i \bracket{ 3 + \frac{16\sigma_i^2 \log n}{\Delta_i^2}}$, which is the analogue of Theorems 7.1 for this case. Define $\sigma_{\max} = \max_{i:\Delta_i>0}\sigma_i$ and set $\Delta = 4\sqrt{\frac{k\sigma_{\max}^2 \log n}{n}}$, we can get the analogue of Theorems 7.2 for this case with $R_n \le 8\sqrt{nk\sigma_{\max}^2 \log n} + 3\sum_{i:\Delta_i >0} \Delta_i$.

    \item[(c)] As shown in part (a) and (b), our regret bound does not depend on the values $\set{\sigma_i^2:\Delta_i=0}$. Due to the monotonicity between UCB$_1$ and $\mu_1$, the number of pulls of a suboptimal arm $i$ only depends on $\sigma_i$ but is not influenced by optimal arms. 
\end{enumerate}
\end{proof}

% \noindent\textbf{7.3}
%     $E[Ti(n)]\le\dfrac{-8ln\delta}{\bigtriangleup_i}+\delta n(n=1)$


%     $R_n=E[\overline{R_n}]\le\sum^{k}_{i=2}\dfrac{-8ln\delta}{\bigtriangleup_i}+\bigtriangleup_i\delta n(n+1)$


%     Choosing
%     $\delta=\dfrac{\sum^{k}_{i=2}\frac{8}{\bigtriangleup_i}}{\sum^{k}_{i=2}\bigtriangleup_i   n(n+1)}$
%     \begin{align*}
%     R_n &\le\sum^{k}_{i=2}\frac{8}{\bigtriangleup_i}+\sum^{k}_{i=2}\frac{8}{\bigtriangleup_i}ln\frac{\sum^{k}_{i=2}\bigtriangleup_i n(n+1)}{\sum^{k}_{i=2}\frac{8}{\bigtriangleup_i}}\\
%     &:=h(n,k)
%     \end{align*}
%     $g(n,k,\delta)=\dfrac{\sqrt{h(n,k)}}{\delta};f(n,k)=\sqrt{h(n,k)}$
%     \begin{align*}
%     P(\overline{R_n}\ge g(n,k,\delta)) &\le\dfrac{R_n}{g(n,k,\delta)}\\
%     &\le\dfrac{h(n,k)}{g(n,k,\delta)}\\
%     &=\dfrac{h(n,k)}{\sqrt{h(n,k)}}\delta\\
%     &=\sqrt{h(n,k)}\delta\\
%     &=f(n,k)\delta
%     \end{align*}

\noindent\textbf{7.4} (\textsc{Phased UCB (i)})
Fix a $1$-subgaussian $k$-armed bandit environment and a horizon $n$.
Consider the version of UCB that works in phases of exponentially increasing length of $1,2,4, \ldots$.
In each phase, the algorithm uses the action that would have been chosen by UCB at the beginning of the phase (see Algorithm 4).

\begin{enumerate}
    \item[(a)] State and prove a bound on the regret for this version of UCB.
    \item[(b)] Compare your result with Theorem 7.1.
    \item[(c)] How would the result change if the $\ell$-th phase had a length of $\left\lceil\alpha^{\ell}\right\rceil$ with $\alpha > 1$?
\end{enumerate}

\begin{proof}
    \begin{enumerate}
        \item[(a)] Let $T_{i}(n)$ denote the number of times pulling arm $i$.
        By definition we have
        $$
        \begin{aligned}
            \mathbb{E}\left[T_{i}(n)\right]
            &=\mathbb{E}\left[ \sum_{t=1}^{n} \mathbb{1}\left\{A_{t}=i\right\}\right]\\
            &=\mathbb{E}\left[ \sum_{\ell=1}^{\ell_{max}} 2^\ell \mathbb{1}\left\{A_{\ell}=i\right\}\right]\\
            &=\mathbb{E}\left[ \sum_{\ell=1}^{\ell_{max}} 2^\ell \mathbb{1}\left\{A_{\ell}=i, \mathrm{UCB}_{i} > \mu_1\right\}\right]\\
            &=\mathbb{E}\left[ \sum_{\ell=1}^{\ell_{max}} 2^\ell \mathbb{1}\left\{A_{\ell}=i, \mu_{i} + \sqrt{\frac{2 \log (1 / \delta)}{T_{i}(t-1)}} > \mu_1\right\}\right]\\
            &=\mathbb{E}\left[ \sum_{\ell=1}^{\ell_{max}} 2^\ell \mathbb{1}\left\{A_{\ell}=i, T_{i}(t-1) < \frac{8 \log \frac{1}{\delta}}{\Delta_{i}^{2}} \right\}\right],\\
        \end{aligned}
        $$
        where $\Delta_i$ is the suboptimality gap.

        To find $\ell_i$ such that $\sum_{\ell=1}^{\ell_{i}} 2^{\ell} \geq \frac{8 \log \frac{1}{\delta}}{\Delta_{i}^{2}}$, we derive that $\ell_i \geq \lceil \log _{2} \frac{4 \log \frac{1}{\delta}}{\Delta_{1}^{2}}+1 \rceil$.
        Hence, we have
        $$
            \mathbb{E}\left[T_{i}(n)\right] \leq \sum_{\ell = 1}^{\ell_i} 2^\ell \leq 32 \frac{\log \frac{1}{\delta}}{\Delta_i^2},
        $$
        Taking $\delta = \frac{1}{n^2}$ results in an upper bound $64\frac{\log n}{\Delta_{i}^{2}}$ for $\mathbb{E}\left[T_{i}(n)\right]$.
    
        \item[(b)] Compared with Theorem 7.1, the constant is larger while the order remains the same.
        
        \item[(c)] With the alternative phase length, the above analysis results in an upper bound of $\mathbb{E}\left[T_{i}(n)\right]$:
        $$
            \mathbb{E}\left[T_{i}(n)\right] \leq \sum_{\ell = 1}^{\ell_i} \alpha^\ell \leq 16\frac{\alpha^{2} \log n}{\Delta_i^{2}},
        $$
        where $\ell_{i}=\left[\log _{\alpha} \frac{8(\alpha-1) \log \frac{1}{\delta}}{\alpha \Delta_i^{2}}+1\right]$.
    \end{enumerate}
\end{proof}

\noindent\textbf{7.6}
    % \item
    \begin{align*}
    \hat{\delta}^2 &=\frac{1}{n}\sum^{n}_{t=1}(\hat{\mu}-X_t)^2\\
    &=\frac{1}{n}\sum^{n}_{t=1}[(\hat{\mu}-\mu)+(\mu-X_t)]^2\\
    &=\frac{1}{n}\sum^{n}_{t=1}(\hat{\mu}-\mu)^2+\frac{1}{n}\sum^{n}_{t=1}(\mu-X-t)^2+\frac{2}{n}\sum^{n}_{t=1}(\hat{\mu}-\mu)(\mu-X_t)
    \end{align*}
    $\because\hat{\mu}-\mu=\frac{1}{n}\sum^{n}_{t=1}X_t-\mu=\frac{1}{n}\sum^{n}_{t=1}(X_t-\mu)$
    \begin{align*}
    \therefore\frac{2}{n}\sum^{n}_{t=1}(\hat{\mu}-\mu)(\mu-X-t) &=\frac{2}{n}\sum^{n}_{t=1}[\frac{1}{n}\sum^{n}_{t=1}(X_t-\mu)](\mu-X_t)\\
    &=-2[\frac{1}{n}\sum^{n}_{t=1}(X_t-\mu)]^2\\
    &=-2[\hat{\mu}-\mu]^2
    \end{align*}
    \begin{align*}
    \hat{\delta}^2 &=(\hat{\mu}-\mu)^2+\frac{1}{n}\sum^{n}_{t=1}(\mu-X_t)^2-2(\hat{\mu}-\mu)^2\\
    &=\frac{1}{n}\sum^{n}_{t=1}(\mu-X_t)^2-(\hat{\mu}-\mu)^2
    \end{align*}
    
    